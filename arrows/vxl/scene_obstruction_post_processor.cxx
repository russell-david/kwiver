// This file is part of KWIVER, and is distributed under the
// OSI-approved BSD 3-Clause License. See top-level LICENSE file or
// https://github.com/Kitware/kwiver/blob/master/LICENSE for details.

#include "scene_obstruction_post_processor.h"

#include <arrows/vxl/image_container.h>

#include <vital/util/enum_converter.h>

#include <vil/vil_convert.h>
#include <vil/vil_image_view.h>
#include <vil/vil_math.h>
#include <vil/vil_pixel_format.h>

#include <deque>

namespace kwiver {

namespace arrows {

namespace vxl {

/// Various learned properties of the scene obstructor for the current frame.
template < typename PixType >
struct scene_obstruction_properties
{
  // Average color of the obstruction
  vil_rgb< PixType > color_{ 0, 0, 0 };

  // Average intensity of the obstructor
  PixType intensity_{ 0 };

  // Did the output mask recently change by a large factor?
  bool break_flag_{ false };

  // Is the information in this structure valid?
  bool is_valid_{ true };
};

/// External settings for the scene_obstructor_detector class.
struct scene_obstruction_detector_settings
{
  /// Main classifier filename
  std::string primary_classifier_filename_;

  /// Appearance-only classifier filename
  std::string appearance_classifier_filename_;

  /// Initial classifier threshold [unitless, depends on model]
  double initial_threshold_;

  /// Should we use a classifier that doesn't use temporal features for
  /// the first n frames?
  bool use_appearance_classifier_;

  /// Number of frames after a break to use the appearance only classifier for.
  unsigned appearance_frames_;

  /// Variance scale factor which maps the input variance image to the
  /// feature type.
  double variance_scale_factor_;

  /// Should we utilize a spatial feature prior? This is typically an image
  /// divided into different segments, although it can also be autogenerated
  /// internally via griding if no file is specified.
  bool use_spatial_prior_feature_;

  /// Filename for spatial prior features, if we want to use an image instead
  /// of an automatically generated image.
  std::string spatial_prior_filename_;

  /// Divides the location feature image into length x length regions, with
  /// each region having a unique ID
  unsigned spatial_prior_grid_length_;

  /// Threshold colors to pure black or pure white only.
  bool map_colors_to_nearest_extreme_;

  /// Threshold colors to pure black or pure white only.
  bool map_colors_near_extremes_only_;

  /// Gray is not a usable color
  bool no_gray_filter_;

  /// Adaptive thresholding parameters
  ///@{
  bool use_adaptive_thresh_;
  unsigned at_pivot_1_;
  unsigned at_pivot_2_;
  double at_interval_1_adj_;
  double at_interval_2_adj_;
  double at_interval_3_adj_;
  ///@}

  /// Mask break detection variables
  ///@{
  bool enable_mask_break_detection_;
  unsigned mask_count_history_length_;
  unsigned mask_intensity_history_length_;
  double count_percent_change_req_;
  double count_std_dev_req_;
  unsigned min_hist_for_intensity_diff_;
  double intensity_diff_req_;
  ///@}

  /// Training data output mode parameters
  ///@{
  bool is_training_mode_;
  bool output_feature_image_mode_;
  bool output_classifier_image_mode_;
  std::string groundtruth_dir_;
  std::string output_filename_;
  ///@}

  // Defaults
  scene_obstruction_detector_settings()
    : primary_classifier_filename_( "" ),
      appearance_classifier_filename_( "" ),
      initial_threshold_( 0.0 ),
      use_appearance_classifier_( true ),
      appearance_frames_( 10 ),
      variance_scale_factor_( 0.32 ),
      use_spatial_prior_feature_( true ),
      spatial_prior_filename_( "" ),
      spatial_prior_grid_length_( 5 ),
      map_colors_to_nearest_extreme_( true ),
      map_colors_near_extremes_only_( false ),
      no_gray_filter_( true ),
      use_adaptive_thresh_( false ),
      at_pivot_1_( 10 ),
      at_pivot_2_( 10 ),
      at_interval_1_adj_( 0.0 ),
      at_interval_2_adj_( 0.0 ),
      at_interval_3_adj_( 0.0 ),
      enable_mask_break_detection_( true ),
      mask_count_history_length_( 20 ),
      mask_intensity_history_length_( 40 ),
      count_percent_change_req_( 3.0 ),
      count_std_dev_req_( 5 ),
      min_hist_for_intensity_diff_( 30 ),
      intensity_diff_req_( 90 ),
      is_training_mode_( false ),
      output_feature_image_mode_( false ),
      output_classifier_image_mode_( false ),
      groundtruth_dir_( "" ),
      output_filename_( "" )
  {}
};


using source_image = vil_image_view< vxl_byte >;
using feature_image = vil_image_view< vxl_byte >;
using feature_array = std::vector< feature_image >;
using mask_type = vil_image_view< bool >;
using classified_image = vil_image_view< double >;
using variance_image = vil_image_view< double >;
using properties = scene_obstruction_properties< vxl_byte >;
//using feature_writer = pixel_feature_writer< FeatureType >;
//using feature_writer_sptr = boost::scoped_ptr< feature_writer >;

// ----------------------------------------------------------------------------
// Private implementation class
class scene_obstruction_post_processor::priv
{
public:
  priv( scene_obstruction_post_processor* parent ) : p{ parent } {}

  // Use different thresholds based on elapsed frames
  double
  adaptive_threshold_contribution( const unsigned& frame_num );
  // TODO Document
  //template < typename PixType, typename FeatureType >
  void
  perform_initial_approximation( const feature_array& features,
                                 classified_image& output_image );
  // Called when a change in obstructions is likely
  void trigger_mask_break( const source_image& input );
  // TODO document
  bool
  estimate_mask_properties( const source_image& input,
                            const classified_image& output );
  // TODO add documentation
  template < typename PixType, typename FeatureType > void
  process_frame( const source_image& input_image,
                 const feature_array& input_features,
                 const variance_image& pixel_variance,
                 classified_image& output_image );

  scene_obstruction_post_processor* p;

  bool is_valid_{ false };

  // Externally set options
  scene_obstruction_detector_settings options_;

  // Internal properties
  properties props_;

  // Internal buffers/counters/classifiers
  unsigned frame_counter_{ 0 };
  unsigned frames_since_last_break_ { 0 };
  double var_hash_scale_{ 0.0 };
  vil_image_view< vxl_byte > var_hash_;
  vil_image_view< vxl_byte > intensity_diff_;
  vil_image_view< double > summed_history_;
  vil_image_view< vxl_byte > spatial_prior_image_;

  // Mask break detection variables
  double cumulative_intensity_{ 0.0 };
  double cumulative_mask_count_{ 0.0 };
  std::deque< double > mask_count_history_;
  std::deque< double > mask_intensity_history_;
  unsigned color_hist_bitshift_{ 0 };
  unsigned color_hist_scale_{ 0 };

  // Variance buffers
  vil_image_view< double > summed_variance_;
  vil_image_view< double > normalized_variance_;

};

// ----------------------------------------------------------------------------
double
scene_obstruction_post_processor::priv
::adaptive_threshold_contribution( const unsigned& frame_num )
{
  if( !options_.use_adaptive_thresh_ )
  {
    return 0.0;
  }
  if( frame_num > options_.at_pivot_2_ )
  {
    return options_.at_interval_3_adj_;
  }
  else if( frame_num > options_.at_pivot_1_ )
  {
    return options_.at_interval_2_adj_;
  }
  return options_.at_interval_1_adj_;
}

// ----------------------------------------------------------------------------
//template < typename PixType, typename FeatureType >
void
scene_obstruction_post_processor::priv
::perform_initial_approximation( const feature_array& features,
                                 classified_image& output_image )
{
  // TODO Figure out what this is doing and whether we should enforce that
  // output image is only one channel
  // Formulate classifier input (vector of single plane byte images)
  vil_image_view< double > approx_output = vil_plane( output_image, 1 );

  // A small (negatively classified) value used for initially filling
  // the output classification. For the most part is not used, except
  // for pixels which don't get processed (ie those outside of the identified
  // border).
  const double negative_fill_value = options_.initial_threshold_ -
                                     std::numeric_limits< double >::epsilon();

  // Seed the output approximate
  approx_output.fill( negative_fill_value );

  // TODO understand this
  // Perform classification
  double offset = adaptive_threshold_contribution( frames_since_last_break_ );

  // Figure out whether we want support for appearance based classification as well
  if( !options_.use_appearance_classifier_ ||
      frames_since_last_break_ > options_.appearance_frames_ )
  {
    initial_classifier_.classify_images( features, approx_output, offset );
  }
  else
  {
    appearance_classifier_.classify_images( features, approx_output, offset );
  }

  // Add classification to history and copy history to output
  // TODO This is the same as approx_output, right?
  vil_image_view< double > statics = vil_plane( output_image, 0 );
  vil_math_image_sum( approx_output, summed_history_, summed_history_ );
  statics.deep_copy( summed_history_ );
}

// ----------------------------------------------------------------------------
void
scene_obstruction_post_processor::priv
::trigger_mask_break( const source_image& input )
{
  summed_history_.set_size( input.ni(), input.nj(), 1 );
  summed_history_.fill( 0 );
  summed_variance_.set_size( input.ni(), input.nj(), 1 );
  summed_variance_.fill( 0 );

  cumulative_intensity_ = 0;
  cumulative_mask_count_ = 0;
  frames_since_last_break_ = 0;

  props_.break_flag_ = true;
}

// ----------------------------------------------------------------------------
// TODO: This should be removed and replaced with some sort of VIL function
// Convert a floating point image to an intergral type by multiplying
// it by a scaling factor in addition to thresholding it in one operation.
// Performs rounding.
template< typename FloatType, typename IntType >
void scale_float_img_to_interval( const vil_image_view<FloatType>& src,
                                  vil_image_view<IntType>& dst,
                                  const FloatType& scale,
                                  const FloatType& max_input_value )
{
  // Resize, cast and copy
  unsigned ni = src.ni(), nj = src.nj(), np = src.nplanes();
  dst.set_size( ni, nj, np );

  std::ptrdiff_t sistep=src.istep(), sjstep=src.jstep(), spstep=src.planestep();
  std::ptrdiff_t distep=dst.istep(), djstep=dst.jstep(), dpstep=dst.planestep();

  IntType max_hashed_value = static_cast<IntType>( max_input_value * scale + 0.5 );

  const FloatType* splane = src.top_left_ptr();
  IntType* dplane = dst.top_left_ptr();
  for (unsigned p=0;p<np;++p,splane += spstep,dplane += dpstep)
  {
    const FloatType* srow = splane;
    IntType* drow = dplane;
    for (unsigned j=0;j<nj;++j,srow += sjstep,drow += djstep)
    {
      const FloatType* spixel = srow;
      IntType* dpixel = drow;
      for (unsigned i=0;i<ni;++i,spixel+=sistep,dpixel+=distep)
      {
        if( *spixel <= max_input_value )
        {
          *dpixel = static_cast<IntType>(*spixel * scale + 0.5);
        }
        else
        {
          *dpixel = max_hashed_value;
        }
      }
    }
  }
}

// ----------------------------------------------------------------------------
// This returns whether a break was detected
bool
scene_obstruction_post_processor::priv
::estimate_mask_properties( const source_image& input,
                            const classified_image& output )
{
  return false;
//#ifdef SOD_DEBUG_IMAGE
//  source_image to_output;
//  to_output.deep_copy( input );
//#endif
//
//  if( input.nplanes() == 3 || input.nplanes() == 1 ){
//    LOG_ERROR( p->logger(), "Invalid plane count" );
//  }
//
//  // Estimate colour and intensity (take a quick cross-channel mode).
//  // Concurrently count # of pixels in the detected mask approximation.
//  const bool is_color_image = ( input.nplanes() == 3 );
//
//  unsigned hist[ 3 ][ hist_bins ] = { { 0 } };
//
//  const std::ptrdiff_t oistep = output.istep();
//  const std::ptrdiff_t ojstep = output.jstep();
//
//  const std::ptrdiff_t ipstep = input.planestep();
//  const std::ptrdiff_t ip2step = 2 * ipstep;
//
//  const double threshold = options_.initial_threshold_;
//
//  // Automatically discount any pixels directly near the estimated border
//  // because they might be warped towards the border color due to compression
//  // artifacts.
//  const unsigned borderadj = 20;
//
//  unsigned init_clfr_counter = 0;
//
//  for( unsigned j = lower_j + borderadj; j < upper_j - borderadj; j++ )
//  {
//    const double* init_clfr_ptr = &output( lower_i + borderadj, j, 1 );
//
//    for( unsigned i = lower_i + borderadj; i < upper_i - borderadj;
//         i++, init_clfr_ptr += oistep )
//    {
//      if( *init_clfr_ptr > threshold )
//      {
//        init_clfr_counter++;
//
//        if( output( i, j, 0 ) > threshold &&
//            calculate_border_degree( init_clfr_ptr, oistep, ojstep, threshold,
//                                     4 ) > 3 &&
//            calculate_long_degree( init_clfr_ptr, oistep, ojstep, threshold, 6,
//                                   3 ) > 2 )
//        {
//          const PixType* color = &input( i, j );
//          hist[ 0 ][ *( color ) >> color_hist_bitshift_ ]++;
//
//#ifdef SOD_DEBUG_IMAGE
//          to_output( i, j, 0 ) = 255;
//          to_output( i, j, 2 ) = 255;
//#endif
//
//          if( is_color_image )
//          {
//            hist[ 1 ][ *( color + ipstep ) >> color_hist_bitshift_ ]++;
//            hist[ 2 ][ *( color + ip2step ) >> color_hist_bitshift_ ]++;
//          }
//        }
//      }
//    }
//  }
//
//#ifdef SOD_DEBUG_IMAGE
//  static int counter = 0;
//  counter++;
//  std::string fn = "frame" + std::to_string( counter ) + ".png";
//  vil_save( to_output, fn.c_str() );
//#endif
//
//  // Calculate mode and cumulative histograms
//  const unsigned div = hist_bins / 3;
//
//  unsigned top_ind[ 3 ] = { 0 };
//  unsigned max_val[ 3 ] = { 0 };
//
//  unsigned cum_hist[ 3 ][ hist_bins ] = { { 0 } };
//  unsigned lmh_sum[ 3 ][ 3 ] = { { 0 } };
//
//  for( unsigned c = 0; c < 3; ++c )
//  {
//    for( unsigned i = 0; i < hist_bins; ++i )
//    {
//      if( hist[ c ][ i ] > max_val[ c ] )
//      {
//        max_val[ c ] = hist[ c ][ i ];
//        top_ind[ c ] = i;
//      }
//
//      if( i != 0 )
//      {
//        cum_hist[ c ][ i ] = cum_hist[ c ][ i - 1 ] + hist[ c ][ i ];
//      }
//    }
//
//    lmh_sum[ c ][ 0 ] = cum_hist[ c ][ div - 1 ];
//    lmh_sum[ c ][ 1 ] = cum_hist[ c ][ hist_bins - 1 - div ] -
//                        cum_hist[ c ][ div - 1 ];
//    lmh_sum[ c ][ 2 ] = cum_hist[ c ][ hist_bins - 1 ] -
//                        cum_hist[ c ][ hist_bins - 1 - div ];
//  }
//
//  // Set color in property options, apply any settings
//  props_.color_.r = top_ind[ 0 ] * color_hist_scale_ + color_hist_scale_ / 2;
//
//  if( is_color_image )
//  {
//    props_.color_.g = top_ind[ 1 ] * color_hist_scale_ + color_hist_scale_ / 2;
//    props_.color_.b = top_ind[ 2 ] * color_hist_scale_ + color_hist_scale_ / 2;
//  }
//  else
//  {
//    props_.color_.g = props_.color_.r;
//    props_.color_.b = props_.color_.r;
//  }
//
//  const unsigned invalid_color = 5;
//  unsigned color_index = invalid_color;
//  unsigned color_weights[ invalid_color ] = { 0 };
//
//  color_weights[ 0 ] = lmh_sum[ 0 ][ 0 ] + lmh_sum[ 1 ][ 0 ] +
//                       lmh_sum[ 2 ][ 0 ];
//  color_weights[ 1 ] = lmh_sum[ 0 ][ 2 ] + lmh_sum[ 1 ][ 2 ] +
//                       lmh_sum[ 2 ][ 2 ];
//  color_weights[ 2 ] = lmh_sum[ 0 ][ 1 ] + lmh_sum[ 1 ][ 1 ] +
//                       lmh_sum[ 2 ][ 1 ];
//
//  if( is_color_image )
//  {
//    color_weights[ 3 ] = lmh_sum[ 0 ][ 0 ] + lmh_sum[ 1 ][ 2 ] +
//                         lmh_sum[ 2 ][ 0 ];
//    color_weights[ 4 ] = lmh_sum[ 0 ][ 0 ] + lmh_sum[ 1 ][ 0 ] +
//                         lmh_sum[ 2 ][ 2 ];
//  }
//
//  if( options_.no_gray_filter_ )
//  {
//    color_weights[ 2 ] = 0;
//  }
//
//  if( options_.map_colors_to_nearest_extreme_ )
//  {
//    color_index = std::distance( color_weights,
//                                 std::max_element( color_weights,
//                                                   color_weights + 4 ) );
//  }
//  else if( options_.map_colors_near_extremes_only_ )
//  {
//    color_index = std::distance( color_weights,
//                                 std::max_element( color_weights,
//                                                   color_weights + 4 ) );
//
//    unsigned max_option = 0;
//
//    for( unsigned i = 0; i < 3; ++i )
//    {
//      for( unsigned j = 0; j < 3; ++j )
//      {
//        for( unsigned k = 0; k < 3; ++k )
//        {
//          unsigned color_count = lmh_sum[ 0 ][ i ] + lmh_sum[ 1 ][ j ] +
//                                 lmh_sum[ 2 ][ k ];
//          max_option = std::max( max_option, color_count );
//        }
//      }
//    }
//
//    if( color_weights[ color_index ] < max_option )
//    {
//      color_index = invalid_color;
//    }
//  }
//
//  switch( color_index )
//  {
//    case 0:
//      props_.color_.r = 0;
//      props_.color_.g = 0;
//      props_.color_.b = 0;
//      break;
//    case 1:
//      props_.color_.r = std::numeric_limits< FeatureType >::max();
//      props_.color_.g = std::numeric_limits< FeatureType >::max();
//      props_.color_.b = std::numeric_limits< FeatureType >::max();
//      break;
//    case 2:
//      props_.color_.r = std::numeric_limits< FeatureType >::max() / 2;
//      props_.color_.g = std::numeric_limits< FeatureType >::max() / 2;
//      props_.color_.b = std::numeric_limits< FeatureType >::max() / 2;
//      break;
//    case 3:
//      props_.color_.r = 0;
//      props_.color_.g = std::numeric_limits< FeatureType >::max();
//      props_.color_.b = 0;
//      break;
//    case 4:
//      props_.color_.r = 0;
//      props_.color_.g = 0;
//      props_.color_.b = std::numeric_limits< FeatureType >::max();
//      break;
//    default:
//      break;
//  }
//
//  props_.intensity_ = props_.color_.grey();
//
//  // Now that we have our recorded properties, enter them into our
//  // history and determine if there may have been a break.
//  //
//  // Feature1 = % difference between the pixel count and the pcount average
//  // Feature2 = # of std devs away from pixel count average
//  // Feature3 = Difference of raw color output and color average
//  double avg = 0.0, dev = 0.0;
//  double pix_count = static_cast< double >( init_clfr_counter );
//
//  if( mask_count_history_.size() == 0 )
//  {
//    mask_count_history_.insert( pix_count );
//    mask_intensity_history_.insert( props_.intensity_ );
//    return false;
//  }
//
//  for( unsigned i = 0; i < mask_count_history_.size(); i++ )
//  {
//    avg += mask_count_history_.datum_at( i );
//  }
//
//  avg /= mask_count_history_.size();
//
//  for( unsigned i = 0; i < mask_count_history_.size(); i++ )
//  {
//    double term = mask_count_history_.datum_at( i ) - avg;
//    dev += term * term;
//  }
//
//  dev = std::sqrt( dev );
//
//  double feature1 = 0.0, feature2 = 0.0;
//
//  if( avg > pix_count )
//  {
//    feature1 = avg / ( pix_count + 1 );
//  }
//  else
//  {
//    feature1 = pix_count / ( avg + 1 );
//  }
//
//  if( dev != 0 )
//  {
//    feature2 = std::fabs( ( pix_count - avg ) / dev );
//  }
//
//  double color_avg = 0.0;
//
//  for( unsigned i = 0; i < mask_intensity_history_.size(); i++ )
//  {
//    color_avg += mask_intensity_history_.datum_at( i );
//  }
//
//  color_avg /= mask_intensity_history_.size();
//
//  // Threshold all features via the given parameters
//  if( feature1 > options_.count_percent_change_req_ ||
//      feature2 > options_.count_std_dev_req_ ||
//      ( mask_intensity_history_.size() >
//        options_.min_hist_for_intensity_diff_ &&
//        std::abs( static_cast< double >( props_.intensity_ ) - color_avg ) >
//        options_.intensity_diff_req_ ) )
//  {
//    mask_count_history_.clear();
//    mask_intensity_history_.clear();
//    mask_count_history_.insert( pix_count );
//    mask_intensity_history_.insert( props_.intensity_ );
//    LOG_INFOR( p->logger(),  "HUD change detected on frame: " << frame_counter_ );
//    return true;
//  }
//
//  mask_count_history_.insert( pix_count );
//  mask_intensity_history_.insert( props_.intensity_ );
//  return false;
}

// ----------------------------------------------------------------------------
template < typename PixType, typename FeatureType >
void
scene_obstruction_post_processor::priv
::process_frame( const source_image& input_image, // Also maybe unneeded
                 const feature_array& input_features, // probably unneeded
                 const variance_image& pixel_variance, // figure out how to grab this
                 classified_image& output_image ) // probably nix that
{
  //if( !is_valid_ )
  //{
  //  LOG_ERROR( p->logger(), "Internal model is not valid!" );
  //}
  //if( input_features.size() <= 0 )
  //{
  //  LOG_ERROR( p->logger(), "No input features provided!" );
  //}
  //if( input_features[ 0 ].ni() != input_image.ni() )
  //{
  //  LOG_ERROR( p->logger(), "Input widths do not match." );
  //}
  //if( input_features[ 0 ].nj() != input_image.nj() )
  //{
  //  LOG_ERROR( p->logger(), "Input heights do not match." );
  //}

  props_.break_flag_ = false;

  // TODO why is this used
  // Figure out how asignment with references works
  feature_array full_feature_array = input_features;

  // Initialize new buffers on the first frame
  if( frame_counter_ == 0 )
  {
    this->trigger_mask_break( input_image );
  }

  // Increment frame counters
  ++frame_counter_;
  ++frames_since_last_break_;

  // TODO see if this makes sense to keep
  // TODO I don't understand why functions like this
  // Update total variance image, since last shot break
  vil_math_image_sum( pixel_variance, summed_variance_, summed_variance_ );

  const double scale_factor = options_.variance_scale_factor_ /
                        frames_since_last_break_;
  const double max_input_value = std::numeric_limits< FeatureType >::max() /
                           scale_factor;
  // TODO consider replacing with a VIL function
  scale_float_img_to_interval( summed_variance_, normalized_variance_,
                               scale_factor, max_input_value );

  // Add variance image and region id image to feature list
  full_feature_array.push_back( normalized_variance_ );

  if( options_.use_spatial_prior_feature_ )
  {
    full_feature_array.push_back( spatial_prior_image_ );
  }

  // Reset output image
  output_image.set_size( input_image.ni(), input_image.nj(), 2 );

  vil_image_view< double > statics = vil_plane( output_image, 0 );
  vil_image_view< double > everything = vil_plane( output_image, 1 );

  // Generate initial per-pixel mask classification
  this->perform_initial_approximation( full_feature_array,
                                       output_image );
  // Really everything up to here is just computing the response

  // Estimate mask properties
  bool break_detected = this->estimate_mask_properties( input_image,
                                                        output_image );

  // Guess what color the mask is, under the assumption its a solid color
  if( options_.enable_mask_break_detection_ && break_detected )
  {
    bool reclassify = options_.use_appearance_classifier_ &&
                      frames_since_last_break_ > options_.appearance_frames_;

    this->trigger_mask_break( input_image );

    // Reperform initial approximation using appearance metrics only
    if( reclassify )
    {
      this->perform_initial_approximation( full_feature_array,
                                           output_image );
    }
  }
}

// ----------------------------------------------------------------------------
scene_obstruction_post_processor
::scene_obstruction_post_processor()
  : d{ new priv{ this } }
{
  attach_logger( "arrows.vxl.scene_obstruction_post_processor" );
}

// ----------------------------------------------------------------------------
scene_obstruction_post_processor
::~scene_obstruction_post_processor()
{
}

// ----------------------------------------------------------------------------
vital::config_block_sptr
scene_obstruction_post_processor
::get_configuration() const
{
  // get base config from base class
  vital::config_block_sptr config = algorithm::get_configuration();

  return config;
}

// ----------------------------------------------------------------------------
void
scene_obstruction_post_processor
::set_configuration( vital::config_block_sptr in_config )
{
  // Start with our generated vital::config_block to ensure that assumed values
  // are present. An alternative would be to check for key presence before
  // performing a get_value() call.
  vital::config_block_sptr config = this->get_configuration();
  config->merge_config( in_config );
}

// ----------------------------------------------------------------------------
bool
scene_obstruction_post_processor
::check_configuration( VITAL_UNUSED vital::config_block_sptr config ) const
{
  return true;
}

// ----------------------------------------------------------------------------
kwiver::vital::image_container_sptr
scene_obstruction_post_processor
::filter( kwiver::vital::image_container_sptr image_data )
{
#define HANDLE_CASE( T )                                         \
  case T:                                                        \
  {                                                              \
    using pix_t = vil_pixel_format_type_of< T >::component_type; \
    vil_image_view< pix_t > input = view;                        \
    return d->process_frame( input );                            \
    break;                                                       \
  }

//  switch( view->pixel_format() )
//  {
//    HANDLE_CASE( VIL_PIXEL_FORMAT_BOOL );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_BYTE );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_SBYTE );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_UINT_16 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_INT_16 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_UINT_32 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_INT_32 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_UINT_64 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_INT_64 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_FLOAT );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_DOUBLE );
//#undef HANDLE_CASE
//
//    default:
//      // The image type was not one we handle
//      LOG_ERROR( logger(), "Unsupported input format " << view->pixel_format()
//                                                       << " type received" );
//      return nullptr;
//  }

  // Code not reached, prevent warning
  return nullptr;
}

} // end namespace vxl

} // end namespace arrows

} // end namespace kwiver
