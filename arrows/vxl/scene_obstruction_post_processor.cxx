// This file is part of KWIVER, and is distributed under the
// OSI-approved BSD 3-Clause License. See top-level LICENSE file or
// https://github.com/Kitware/kwiver/blob/master/LICENSE for details.

#include "scene_obstruction_post_processor.h"

#include <arrows/vxl/image_container.h>

#include <vital/util/enum_converter.h>

#include <vil/vil_convert.h>
#include <vil/vil_image_view.h>
#include <vil/vil_math.h>
#include <vil/vil_pixel_format.h>

namespace kwiver {

namespace arrows {

namespace vxl {

/// Various learned properties of the scene obstructor for the current frame.
template < typename PixType >
struct scene_obstruction_properties
{
  // Average color of the obstruction
  vil_rgb< PixType > color_{ 0, 0, 0 };

  // Average intensity of the obstructor
  PixType intensity_{ 0 };

  // Did the output mask recently change by a large factor?
  bool break_flag_{ false };

  // Is the information in this structure valid?
  bool is_valid_{ true };
};

/// External settings for the scene_obstructor_detector class.
struct scene_obstruction_detector_settings
{
  /// Main classifier filename
  std::string primary_classifier_filename_;

  /// Appearance-only classifier filename
  std::string appearance_classifier_filename_;

  /// Initial classifier threshold [unitless, depends on model]
  double initial_threshold_;

  /// Should we use a classifier that doesn't use temporal features for
  /// the first n frames?
  bool use_appearance_classifier_;

  /// Number of frames after a break to use the appearance only classifier for.
  unsigned appearance_frames_;

  /// Variance scale factor which maps the input variance image to the
  /// feature type.
  double variance_scale_factor_;

  /// Should we utilize a spatial feature prior? This is typically an image
  /// divided into different segments, although it can also be autogenerated
  /// internally via griding if no file is specified.
  bool use_spatial_prior_feature_;

  /// Filename for spatial prior features, if we want to use an image instead
  /// of an automatically generated image.
  std::string spatial_prior_filename_;

  /// Divides the location feature image into length x length regions, with
  /// each region having a unique ID
  unsigned spatial_prior_grid_length_;

  /// Threshold colors to pure black or pure white only.
  bool map_colors_to_nearest_extreme_;

  /// Threshold colors to pure black or pure white only.
  bool map_colors_near_extremes_only_;

  /// Gray is not a usable color
  bool no_gray_filter_;

  /// Adaptive thresholding parameters
  ///@{
  bool use_adaptive_thresh_;
  unsigned at_pivot_1_;
  unsigned at_pivot_2_;
  double at_interval_1_adj_;
  double at_interval_2_adj_;
  double at_interval_3_adj_;
  ///@}

  /// Mask break detection variables
  ///@{
  bool enable_mask_break_detection_;
  unsigned mask_count_history_length_;
  unsigned mask_intensity_history_length_;
  double count_percent_change_req_;
  double count_std_dev_req_;
  unsigned min_hist_for_intensity_diff_;
  double intensity_diff_req_;
  ///@}

  /// Training data output mode parameters
  ///@{
  bool is_training_mode_;
  bool output_feature_image_mode_;
  bool output_classifier_image_mode_;
  std::string groundtruth_dir_;
  std::string output_filename_;
  ///@}

  // Defaults
  scene_obstruction_detector_settings()
    : primary_classifier_filename_( "" ),
      appearance_classifier_filename_( "" ),
      initial_threshold_( 0.0 ),
      use_appearance_classifier_( true ),
      appearance_frames_( 10 ),
      variance_scale_factor_( 0.32 ),
      use_spatial_prior_feature_( true ),
      spatial_prior_filename_( "" ),
      spatial_prior_grid_length_( 5 ),
      map_colors_to_nearest_extreme_( true ),
      map_colors_near_extremes_only_( false ),
      no_gray_filter_( true ),
      use_adaptive_thresh_( false ),
      at_pivot_1_( 10 ),
      at_pivot_2_( 10 ),
      at_interval_1_adj_( 0.0 ),
      at_interval_2_adj_( 0.0 ),
      at_interval_3_adj_( 0.0 ),
      enable_mask_break_detection_( true ),
      mask_count_history_length_( 20 ),
      mask_intensity_history_length_( 40 ),
      count_percent_change_req_( 3.0 ),
      count_std_dev_req_( 5 ),
      min_hist_for_intensity_diff_( 30 ),
      intensity_diff_req_( 90 ),
      is_training_mode_( false ),
      output_feature_image_mode_( false ),
      output_classifier_image_mode_( false ),
      groundtruth_dir_( "" ),
      output_filename_( "" )
  {}
};


using source_image = vil_image_view< vxl_byte >;
using feature_image = vil_image_view< vxl_byte >;
using feature_array = std::vector< feature_image >;
using mask_type = vil_image_view< bool >;
using classified_image = vil_image_view< double >;
using variance_image = vil_image_view< double >;
using properties = scene_obstruction_properties< vxl_byte >;
//using feature_writer = pixel_feature_writer< FeatureType >;
//using feature_writer_sptr = boost::scoped_ptr< feature_writer >;

// ----------------------------------------------------------------------------
// Private implementation class
class scene_obstruction_post_processor::priv
{
public:
  priv( scene_obstruction_post_processor* parent ) : p{ parent } {}

  // ----------------------------------------------------------------------------
  template < typename PixType, typename FeatureType >
  void
  process_frame( const source_image& input_image,
                 const feature_array& input_features,
                 const variance_image& pixel_variance,
                 classified_image& output_image,
                 properties& output_properties );

  scene_obstruction_post_processor* p;

  bool is_valid_{ false };

  // Externally set options
  scene_obstruction_detector_settings options_;

  // Internal properties
  properties props_;

  // Internal buffers/counters/classifiers
  hashed_image_classifier< FeatureType > initial_classifier_;
  hashed_image_classifier< FeatureType > appearance_classifier_;
  unsigned frame_counter_{ 0 };
  unsigned frames_since_last_break_; { 0 }
  double var_hash_scale_{ 0.0 };
  vil_image_view< PixType > var_hash_;
  vil_image_view< PixType > intensity_diff_;
  vil_image_view< double > summed_history_;
  vil_image_view< FeatureType > spatial_prior_image_;

  // Mask break detection variables
  double cumulative_intensity_{ 0.0 };
  double cumulative_mask_count_{ 0.0 };
  vidtk::ring_buffer< double > mask_count_history_;
  vidtk::ring_buffer< double > mask_intensity_history_;
  unsigned color_hist_bitshift_{ 0 };
  unsigned color_hist_scale_{ 0 };

  // Training mode variables
  feature_writer_sptr training_data_extractor_;

  // Variance buffers
  vil_image_view< double > summed_variance_;
  vil_image_view< PixType > normalized_variance_;

};


// ----------------------------------------------------------------------------
template < typename PixType, typename FeatureType >
void
scene_obstruction_post_processor::priv
::process_frame( const source_image& input_image,
                 const feature_array& input_features,
                 const variance_image& pixel_variance,
                 classified_image& output_image,
                 properties& output_properties )
{
  if( !is_valid_ )
  {
    LOG_ERROR( p->logger(), "Internal model is not valid!" );
  }
  if( input_features.size() <= 0 )
  {
    LOG_ERROR( p->logger(), "No input features provided!" );
  }
  if( input_features[ 0 ].ni() != input_image.ni() )
  {
    LOG_ERROR( p->logger(), "Input widths do not match." );
  }
  if( input_features[ 0 ].nj() != input_image.nj() )
  {
    LOG_ERROR( p->logger(), "Input heights do not match." );
  }

  props_.break_flag_ = false;

  // TODO why is this used
  feature_array full_feature_array = input_features;

  // Initialize new buffers on the first frame
  if( frame_counter_ == 0 )
  {
    this->trigger_mask_break( input_image );

    // TODO look into this
    if( options_.use_spatial_prior_feature_ )
    {
      this->configure_spatial_prior( input_image );
    }
  }

  // Increment frame counters
  ++frame_counter_;
  ++frames_since_last_break_;

  // TODO address whether this is wrong
  // Update total variance image (since last shot break)
  vil_math_image_sum( pixel_variance, summed_variance_, summed_variance_ );

  const double scale_factor = options_.variance_scale_factor_ /
                        frames_since_last_break_;
  const double max_input_value = std::numeric_limits< FeatureType >::max() /
                           scale_factor;
  scale_float_img_to_interval( summed_variance_, normalized_variance_,
                               scale_factor, max_input_value );

  // Add variance image and region id image to feature list
  full_feature_array.push_back( normalized_variance_ );

  if( options_.use_spatial_prior_feature_ )
  {
    full_feature_array.push_back( spatial_prior_image_ );
  }

  // Reset output image
  output_image.set_size( input_image.ni(), input_image.nj(), 2 );

  vil_image_view< double > statics = vil_plane( output_image, 0 );
  vil_image_view< double > everything = vil_plane( output_image, 1 );

  // Optional debug feature output
  if( options_.output_feature_image_mode_ )
  {
    this->output_feature_images( full_feature_array );
  }

  // If in training mode, extract feature data
  if( options_.is_training_mode_ )
  {
    // TODO What is getting written here?
    this->training_data_extractor_->step( full_feature_array );
    return;
  }

  // Generate initial per-pixel mask classification
  this->perform_initial_approximation( full_feature_array,
                                       input_border,
                                       output_image );

  // Estimate mask properties
  bool break_detected = this->estimate_mask_properties( input_image,
                                                        input_border,
                                                        output_image );

  // Guess what color the mask is, under the assumption its a solid color
  if( options_.enable_mask_break_detection_ && break_detected )
  {
    bool reclassify = options_.use_appearance_classifier_ &&
                      frames_since_last_break_ > options_.appearance_frames_;

    this->trigger_mask_break( input_image );

    // Reperform initial approximation using appearance metrics only
    if( reclassify )
    {
      this->perform_initial_approximation( full_feature_array,
                                           input_border,
                                           output_image );
    }
  }

  // Output weighted classification image for each feature
  if( options_.output_classifier_image_mode_ )
  {
    this->output_classifier_images( full_feature_array,
                                    input_border,
                                    output_image );
  }

  // Set output properties
  output_properties = props_;
}



// ----------------------------------------------------------------------------
scene_obstruction_post_processor
::scene_obstruction_post_processor()
  : d{ new priv{ this } }
{
  attach_logger( "arrows.vxl.scene_obstruction_post_processor" );
}

// ----------------------------------------------------------------------------
scene_obstruction_post_processor
::~scene_obstruction_post_processor()
{
}

// ----------------------------------------------------------------------------
vital::config_block_sptr
scene_obstruction_post_processor
::get_configuration() const
{
  // get base config from base class
  vital::config_block_sptr config = algorithm::get_configuration();

  return config;
}

// ----------------------------------------------------------------------------
void
scene_obstruction_post_processor
::set_configuration( vital::config_block_sptr in_config )
{
  // Start with our generated vital::config_block to ensure that assumed values
  // are present. An alternative would be to check for key presence before
  // performing a get_value() call.
  vital::config_block_sptr config = this->get_configuration();
  config->merge_config( in_config );
}

// ----------------------------------------------------------------------------
bool
scene_obstruction_post_processor
::check_configuration( VITAL_UNUSED vital::config_block_sptr config ) const
{
  return true;
}

// ----------------------------------------------------------------------------
kwiver::vital::image_container_sptr
scene_obstruction_post_processor
::filter( kwiver::vital::image_container_sptr image_data )
{
#define HANDLE_CASE( T )                                         \
  case T:                                                        \
  {                                                              \
    using pix_t = vil_pixel_format_type_of< T >::component_type; \
    vil_image_view< pix_t > input = view;                        \
    return d->process_frame( input );                            \
    break;                                                       \
  }

//  switch( view->pixel_format() )
//  {
//    HANDLE_CASE( VIL_PIXEL_FORMAT_BOOL );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_BYTE );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_SBYTE );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_UINT_16 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_INT_16 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_UINT_32 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_INT_32 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_UINT_64 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_INT_64 );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_FLOAT );
//    HANDLE_CASE( VIL_PIXEL_FORMAT_DOUBLE );
//#undef HANDLE_CASE
//
//    default:
//      // The image type was not one we handle
//      LOG_ERROR( logger(), "Unsupported input format " << view->pixel_format()
//                                                       << " type received" );
//      return nullptr;
//  }

  // Code not reached, prevent warning
  return nullptr;
}

} // end namespace vxl

} // end namespace arrows

} // end namespace kwiver
